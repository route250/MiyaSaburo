import sys,os,re
import openai, tiktoken
from openai.embeddings_utils import cosine_similarity
import requests
from PIL import Image
from io import BytesIO
from tools.webSearchTool import WebSearchModule
from libs.utils import Utils

def to_embedding( input ):
    res = openai.Embedding.create(input=input, model="text-embedding-ada-002")
    return [data.get('embedding',None) for data in res.get('data',[])]

tk_encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")

def fsplit_contents( text_all, split_size=2000, oerlap=200 ):
    tk_all = tk_encoder.encode(text_all)
    size_all =  len(tk_all)
    p = 0
    step = 2000
    overlap = 200
    results = []
    while p<size_all:
        info_list = tk_all[p:p+step]
        contents = tk_encoder.decode(info_list)
        results.append( (len(info_list),contents) )
        p = p + step - overlap
    return results

def neko_neko_network():
    #baselist=["çŒ«ãŒè¿·å­","çŒ«ãŒè¡Œæ–¹ä¸æ˜","çŒ«ãŒå®¶å‡º","çŒ«ãŒå¸°ã£ã¦ãã¾ã—ãŸ","çŒ«ã‚’æ¢ã—ã¦"]
    #base_emg_list  = to_embedding(baselist)
    
    module : WebSearchModule = WebSearchModule()
    dates = [ Utils.date_today(), Utils.date_today(-1), Utils.date_today(-2)]

    query = f"( çŒ« OR ã­ã“ OR ãƒã‚³) ( è¿·å­ OR è„±èµ° OR è¡Œæ–¹ä¸æ˜ )"
    query = f"( çŒ« OR ã­ã“ OR ãƒã‚³) ( è¿·å­ OR è„±èµ° OR è¡Œæ–¹ä¸æ˜ ) ( {' OR '.join(dates)} )"

    n_return = 10
    search_results = module.search_meta( query, num_result = n_return )
    print( f"result:{len(search_results)}")


    prompt = "ä¸Šè¨˜ã®æ–‡ç« ã‹ã‚‰è¿·ã„çŒ«ã®æƒ…å ±ã‚’ä»¥ä¸‹ã®è¡¨ã«ã¾ã¨ã‚ã¦ä¸‹ã•ã„ã€‚"
    table = "|æ—¥ä»˜|å ´æ‰€|çŒ«ã®åå‰|çŒ«ã®ç‰¹å¾´|çŠ¶æ³ã€è¡Œæ–¹ä¸æ˜,è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ,é‡Œè¦ªå‹Ÿé›†,é‡Œè¦ªæ±ºå®š|ãã®ä»–|\n|----|----|----|----|----|\n|2023-08-03|æ±äº¬éƒ½å¤šæ‘©å·åŒº|ã¿ãƒ¼ã¡ã‚ƒã‚“|çœŸã£ç™½ãªæ¯›ä¸¦ã¿|è¡Œæ–¹ä¸æ˜|ãµã‚‰ã£ã¨å‡ºæ›ã‘ãŸãã‚Šå¸°ã£ã¦æ¥ã¾ã›ã‚“|\n|2023-08-03|å¤§é˜ªåºœè¥¿æˆåŒº|ã“ã¦ã¤|è™ç¸|è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ|é›£æ³¢ã§é£²ã¿ã¤ã¶ã‚Œã¦ã„ã‚‹ã¨ã“ã‚ã‚’ä¿è­·ã—ã¾ã—ãŸ|\n"
    prompt = "ä¸Šè¨˜ã®æ–‡ç« ã‹ã‚‰è¿·ã„çŒ«ã®æƒ…å ±ã‚’ä¸‹è¨˜ã®ä¾‹ã«å¾“ã£ã¦æŠ½å‡ºã—ã¦ä¸‹ã•ã„ã€‚"
    filters = {
        "æ—¥ä»˜": "2023/08/04 åˆã¯ ä¸æ˜",
        "çŠ¶æ³":"è¡Œæ–¹ä¸æ˜ã€è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€é‡Œè¦ªå‹Ÿé›†ã€é‡Œè¦ªæ±ºå®šã€ãªã©",
        "å ´æ‰€":"å¸‚ç”ºæ‘ãªã©",
        "åå‰":"çŒ«ã®åå‰ åˆã¯ æœªå®š",
        "å“ç¨®":"é›‘ç¨® ãƒ™ãƒ³ã‚¬ãƒ«ç­‰",
        "æ¯›è‰²":"ä¸‰æ¯›ã€ç™½ã€é»’ã€ã¨ã‚‰ç¸ãªã©",
        "æ€§åˆ¥":"ã‚ªã‚¹ åˆã¯ ãƒ¡ã‚¹",
        "å¹´é½¢":"æ¨å®šï¼‘æ­³ï¼˜ãƒ¶æœˆ",
        "ãã®ä»–ç‰¹å¾´":"å‘¼ã³ã‹ã‘ã‚‹ã¨ã€ŒãŠã£ã™ã€ã¨è¿”äº‹ã—ã¾ã™ã€‚ã‚»ãƒŸã‚’å–ã‚Šã«è¡Œãã¨å‡ºã¦ã„ã£ãŸãã‚Šå¸°ã£ã¦ãã¾ã›ã‚“",
        "æ²è¼‰ã‚µã‚¤ãƒˆ":"æƒ…å ±ã¸ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°è¨˜è¼‰" 
    }
    table ="ä¾‹)"
    for k,v in filters.items():
        table = f"{table}\n{k}: {v}"

    info_list = []

    for d in search_results:
        site_title = d.get('title',"")
        site_link = d.get('link',"")
        if len(site_title)==0 or len(site_link)==0:
            continue
        print("----")
        print( f"{site_title} {site_link}")
        site_text = module.get_content( site_link )
        if site_text is None:
            continue
        print( f"--- split" )
        split_contents = fsplit_contents( site_text )
        idx = 0
        for tokens, context in split_contents:
            idx += 1
            print(f"--- completion:{idx}/{len(split_contents)}")
            try:
                completion = openai.ChatCompletion.create( model = "gpt-3.5-turbo",  
                        messages = [
                            { "role":"user", "content": prompt + "\n\n\n" + context+ "\n\n\n" + prompt + "\n\n" + table },
                            # { "role":"system", "content": f"{prompt}\n{table}" },
                        ],
                        max_tokens  = 3400-tokens, n = 1, stop = None, temperature = 0.0, 
                )
            except Exception as ex:
                print(ex)
                continue
            
            # å¿œç­”
            response = completion.choices[0].message.content
            # é›†è¨ˆ
            print(f"--- parse:{idx}/{len(split_contents)}")
            info = {}
            for line in response.split("\n"):
                kv = line.split(':',1)
                key: str = None
                value: str = ''
                if len(kv)==2:
                    key = kv[0].strip()
                    value = kv[1].strip()
                    if key=="æ—¥ä»˜":
                        value = Utils.date_from_str(value)
                    elif not value or "ä¸æ˜"==value or "æœªå®š"==value:
                        value = ''
                if key and key in filters:
                    if value and len(value)>0:
                        print( f"hit    {line}")
                        info[key] = value
                    else:
                        print( f"skip   {line}")
                else:
                        print( f"ignore {line}")
                        if len(info)>0 and "æ—¥ä»˜" in info:
                            info['title'] = site_title
                            info['link'] = site_link
                            info_list.append(info)
                            info = {}
            if len(info)>0:
                info_list.append(info)
                info = {}
    for info in info_list:
        if info.get('æ—¥ä»˜',"") in dates:
            print("\n\n")
            print(info)

def neko_news():
    
    module : WebSearchModule = WebSearchModule()
    dates = [ Utils.date_today(), Utils.date_today(-1), Utils.date_today(-2)]

    dates = "2023/8/21"
    query = f"çŒ«ã®è©±é¡Œ after: {dates}"


    n_return = 10
    search_results = module.search_meta( query, num_result = n_return )
    # print( f"result:{len(search_results)}")

    for d in search_results:
        site_title = d.get('title',"")
        site_link = d.get('link',"")
        if len(site_title)==0 or len(site_link)==0:
            print( f"Error: no title {site_title} {site_link}")
            continue
        if "youtube.com" in site_link:
            print( f"Error: skip youtube {site_title} {site_link}")
            continue

        site_text: str = module.get_content( site_link, type="title" )
        if site_text is None or site_text == '':
            print( f"Error: no article {site_title} {site_link}")
            continue

        if site_text.find("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")>=0 or site_text.find("å…¬é–‹æœŸé–“ãŒçµ‚äº†")>=0:
            print( f"Error: no article {site_title} {site_link}")
            print(site_text)
            continue

        examples = [ 
            ("ãƒˆã‚«ã‚²è¦‹ã¤ã‘ã¦ä¸‹ã•ã„","ãªã‚“ã‹ã®åºƒå‘Š\nãƒˆã‚«ã‚²ãŒæ˜¨æ—¥é€ƒã’ã¾ã—ãŸ\nãªã‚“ã‹ã®åºƒå‘Š","ãƒˆã‚«ã‚²ãŒé€ƒã’ã¾ã—ãŸ"),
            ("ãƒˆã‚«ã‚²è¦‹ã¤ã‘ã¦ä¸‹ã•ã„","ãªã‚“ã‹ã®åºƒå‘Š\nãªã‚“ã‹ã®åºƒå‘Š\nãªã‚“ã‹ã®åºƒå‘Š","è¨˜äº‹ç„¡ã—"),
            ]
        prompt_fmt = "ä¸‹è¨˜ã®è¨˜äº‹ã‚’ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã«ç´¹ä»‹ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã‚’256æ–‡å­—ä»¥å†…ã§ç”Ÿæˆã—ã¦ä¸‹ã•ã„ã€‚\næœ‰åŠ¹ãªè¨˜äº‹ãŒãªã„å ´åˆã€è¨˜äº‹ç„¡ã—ã¨å›ç­”ã™ã‚‹ã“ã¨ã€‚"
        article_fmt = "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{}\nè¨˜äº‹å†…å®¹:\n{}"
        msg_hist = []
        # for title_data,in_data, out_data in examples:
        #     msg_hist += [ {"role": "system", "content": prompt_fmt.format(title_data) } ]
        #     msg_hist += [ {"role": "user", "content": in_data } ]
        #     msg_hist += [ {"role": "assistant", "content": out_data } ]

        msg_hist += [ {"role": "system", "content": prompt_fmt } ]
        msg_hist += [ {"role": "user", "content": article_fmt.format( site_title, site_text[:2000] ) } ]

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=msg_hist
        )

        if response is None or response.choices is None or len(response.choices)==0:
            print( f"Error:invalid response from openai\n{response}")
            continue

        base_article = response.choices[0]["message"]["content"].strip()
        
        if base_article is None or len(base_article)<20:
            print( f"Error: no tweet {site_title} {site_link}\n{base_article}")
            continue
        tweet_text = base_article
        p = tweet_text.find("#")
        hashtag_list = [ "#çŒ«", "#cat", "#çŒ«å¥½ãã•ã‚“ã¨ç¹‹ãŒã‚ŠãŸã„"]
        if p>1:
            tweet_text = tweet_text[:p].strip()
        tweet_tags = " ".join(hashtag_list)
        
        print("----------------------------------------------")
        print( f"{tweet_text}" )
        print( f"{site_link}")
        print( f"{tweet_tags}" )
        print("----------------------------------------------")

# DALL-Eã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ
def generate_image(prompt):
    response = openai.Image.create(
        model="image-alpha-001",
        prompt=prompt,
        n=1,
        size="256x256",
        response_format="url"
    )

    image_url = response['data'][0]['url']
    # ç”»åƒã‚’è¡¨ç¤º
    response = requests.get(image_url)
    img = Image.open(BytesIO(response.content))
    return img

def img_test():
    # èµ¤ã„ãƒªãƒ³ã‚´ã®ç”»åƒã‚’ç”Ÿæˆ
    image = generate_image("ã€Œã«ã‚ƒã‚“ï¼æœ€æ–°ã®çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ãŸãƒ‹ãƒ£ï¼é»’çŒ«ãŒå¢¨æ±ã«æ“¬æ…‹ã—ã¦ã‚‹ãƒ‹ãƒ£ã‚“ã¦ï¼é©šãã®ç¾ã—ã•ã«ã«ã‚ƒã‚“ã¨ã‚‚è¨€ãˆãªã„ãƒ‹ãƒ£ï¼ãœã²ã¿ã‚“ãªã«çŸ¥ã‚‰ã›ãŸã„ãƒ‹ãƒ£ï¼ğŸ˜ºğŸ“°ğŸ–¤ #çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ #ãƒ‹ãƒ£ãƒ³ãƒ¢ãƒŠã‚¤ãƒˆã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼šçŒ«ã¯æ¶²ä½“â†’ã§ã¯é»’çŒ«ãŒæ¶²ä½“åŒ–ã™ã‚‹ã¨â€¦ï¼Ÿãªã‚“ã¨å¢¨æ±ã«ãªã‚‹ã“ã¨ãŒ")
    image.show()

def test():
    s = "2023-0a8-10"
    ss = Utils.date_from_str(s)
    print(f"{s} --> {ss}")
    d0 = Utils.date_today()
    print(f" d0: {d0}")
    d1 = Utils.date_today(-1)
    print(f" d1: {d1}")
    d2 = Utils.date_today(-2)
    print(f" d2: {d2}")

if __name__ == '__main__':
    #sys.exit(main(sys.argv))
    #xtest()
    neko_news()
    #img_test()
    #test()

