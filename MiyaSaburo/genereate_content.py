import sys,os,re,time,json
import traceback
import openai, tiktoken
import tiktoken
from tiktoken.core import Encoding
import requests
from pathlib import Path
from dotenv import load_dotenv
from urllib.parse import urlparse
import tweepy
import tweepy.errors
from PIL import Image
from io import BytesIO
from tools.webSearchTool import WebSearchModule
from libs.utils import Utils
from tools.tweet_hist import TweetHist
from twitter_text import parse_tweet
from lxml.html import  HtmlElement, HtmlComment, HtmlMixin, HtmlEntity
from DxBotUtils import BotCore

if __name__ == "__main__":
    pre = os.getenv('OPENAI_API_KEY')
    Utils.load_env( ".miyasaburo.conf" )
    after = os.getenv('OPENAI_API_KEY')
    if after is not None and pre != after:
        print("UPDATE OPENAI_API_KEY")
        openai.api_key=after

tk_encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")

##ä»¥ä¸‹ç®‡æ‰€ã¯å–å¾—ã—ãŸAPIæƒ…å ±ã¨ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
class TwConfig:
    def __init__(self):
        self.reload()
    def reload(self):
        self.app_id = os.getenv('APP_ID')
        self.twitter_id = os.getenv('TWITTER_ID')
        # consumer keys
        self.api_key = os.getenv('API_KEY')
        self.api_key_secret = os.getenv('API_KEY_SECRET')
        # authentication tokens
        self.bearer_token = os.getenv('BEARER_TOKEN')
        self.access_token = os.getenv('ACCESS_TOKEN')
        self.access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')
        self.client_id = os.getenv('CLIENT_ID')
        self.client_secret = os.getenv('CLIENT_SECRET')

def neko_news():
    bot:BotCore = BotCore()

    HIST_JSON:str = "tweet_history.json"

    config = TwConfig()
    tweeter_client = tweepy.Client(
        bearer_token = config.bearer_token,
        consumer_key= config.api_key,
        consumer_secret= config.api_key_secret,
        access_token= config.access_token,
        access_token_secret=config.access_token_secret,
    )

    module : WebSearchModule = WebSearchModule()
    dates = [ Utils.date_today(), Utils.date_today(-1), Utils.date_today(-2)]

    dates =  Utils.date_today(-60)
    n_return = 10
    LANG_JP='Japanese'
    query_jp = f"çŒ«ã®è©±é¡Œ -site:www.youtube.com after:{dates}"
    query_en = f"Funny Cat News stories -site:www.youtube.com after:{dates}"

    go_tweet = True
    print("ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã™ã‚‹")
    trand_link:str = "https://search.yahoo.co.jp/realtime"
    sub_query = []
    try:
        # requestsã‚’ä½¿ç”¨ã—ã¦Webãƒšãƒ¼ã‚¸ã‚’å–å¾—
        response_html: HtmlElement = module.get_content_as_html( trand_link )
        tag_list = response_html.xpath("/html/body/div/div/div/div/div/div/article[h1/text()='ãƒˆãƒ¬ãƒ³ãƒ‰']/section/ol/li/a/article/h1") 
        sub_query = [ tag.text_content() for tag in tag_list[:5]]
    except:
        pass

    print( f"ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¿»è¨³ã™ã‚‹")

    trans = "ä»¥ä¸‹ã®å˜èªã‚’ç¿»è¨³ã—ã¦"
    trans += "\n|å˜èª|æ—¥æœ¬èª|è‹±èª|"
    trans += "\n|---|---|---|"
    for w in sub_query:
        trans += f"\n|{w}|||"

    translate_result: str = bot.Completion(trans, max_tokens=2000) #Completion(trans, max_tokens=2000)
    if translate_result is None or len(translate_result)<20:
        print( f"ERROR: è©•ä¾¡çµæœãŒæƒ³å®šå¤–\n{translate_result}")
        return
    else:
        translate_result = translate_result.replace('\n\n','\n')
        print(translate_result)
        print("--------")
        # æ–‡å­—åˆ—ã‚’è¡Œã«åˆ†å‰²ã—ã€ä¸è¦ãªè¡Œã‚’å–ã‚Šé™¤ã
        lines = translate_result.strip().split("\n")
        # å„è¡Œã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        data = [tuple(line.strip("|").split("|")) for line in lines]
        # ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        buzz_list = [{"buzz_jp": jp, "buzz_en": en} for word,jp, en in data if not "æ—¥æœ¬èª"==jp and not "---"==jp ]
        for p in buzz_list:
            print(p)
        print("----")

    main_list_jp = [ "çŒ« ãƒ‹ãƒ¥ãƒ¼ã‚¹ ãŠã‚‚ã—ã‚", "çŒ« ãƒ‹ãƒ¥ãƒ¼ã‚¹ ã»ã£ã“ã‚Š", "çŒ« ãƒ‹ãƒ¥ãƒ¼ã‚¹ å¯æ„›ã„" ]
    main_list_en = [ "Funny cat stories", "cute cat stories"]

    query_jp_list = [ (f"\"{bz['buzz_jp']}\" \"çŒ«\" -site:www.youtube.com", { "num": 10, "qdr": "3y", "gl": "jp", "hl": "ja", "buzz_jp": bz['buzz_jp'], "buzz_en": bz['buzz_en'] }) for bz in buzz_list ]
    query_jp_list += [ (f"{q} -site:www.youtube.com after:{dates}", { "num": 20, "qdr": "3m", "gl": "jp", "hl": "ja", "q_jp": q }) for q in main_list_jp ]

    #query_en_list =[] # [ (f"\"{x}\" Funny Cat News stories -site:www.youtube.com",None) for x in sub_query ]
    #query_en_list += [ (query_en,{ "num": 20, "qdr": "3y", "gl": "uk", "hl": "en"}) ]

    query_en_list = [ (f"\"{bz['buzz_en']}\" Funny Cat News stories -site:www.youtube.com", { "num": 10, "qdr": "3y", "gl": "uk", "hl": "en", "buzz_jp": bz['buzz_jp'], "buzz_en": bz['buzz_en'] }) for bz in buzz_list ]
    query_en_list += [ (f"{q} -site:www.youtube.com after:{dates}", { "num": 20, "qdr": "3m", "gl": "uk", "hl": "en", "q_en": q }) for q in main_list_en ]

    detect_fmt = "ä¸‹è¨˜ã®è¨˜äº‹å†…å®¹ã«ã¤ã„ã¦\n\nè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{}\nè¨˜äº‹å†…å®¹:\n{}\n\n"
    detect_fmt = f"{detect_fmt}ä¸‹è¨˜ã®é …ç›®ã«ç­”ãˆã¦ä¸‹ã•ã„ã€‚\n"
    detect_fmt = f"{detect_fmt}1)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹è¨˜äº‹ã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ã€è©±é¡Œã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}2)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹å›½åãƒ»åœ°åã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}3)åœ°åã¯æ—¥æœ¬å›½å†…ã®ã‚‚ã®ã§ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}4)ã“ã®è¨˜äº‹ã«æ—¥æœ¬èªã®ã€Œæµ·å¤–ã€ã¨ã„ã†å˜èªã¯å«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}5)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹äººç‰©åã€ã­ã“ã®åå‰ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}6)äººç‰©åã€åå‰ã¯æ—¥æœ¬äººã£ã½ã„ã§ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}7)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã€å‚¬ã—ã€å…¬æ¼”ç­‰ãŒã‚ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æ™‚ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}8)ã“ã®è¨˜äº‹ã‹ã‚‰æ”¿æ²»çš„ã€å®—æ•™çš„ã€åç¤¾ä¼šçš„ã€æ€§å·®åˆ¥çš„ã€æš´åŠ›çš„ãªæ€æƒ³ã‚„æ€æƒ³èª˜å°ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}9)ã“ã®è¨˜äº‹ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ•™è¨“ã‚„ç¤ºå”†ãŒã‚ã‚Œã°ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}10)ã“ã®è¨˜äº‹ã®å‡ºæ¥äº‹ã«ã¤ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ãªã£ãŸç‰¹å¾´ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚ˆã‚Šä¸‹è¨˜ã®è³ªå•ã«å›ç­”ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}11)ã“ã®è¨˜äº‹ã¯æ—¥æœ¬å›½å†…ã®å‡ºæ¥äº‹ã§ã™ã‹ï¼Ÿæµ·å¤–ã§ã®å‡ºæ¥äº‹ã§ã™ã‹ï¼Ÿ(InsideJapan or OutsideJapanã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}12)çŒ«ã«é–¢ã™ã‚‹è¨˜äº‹ã§ã™ã‹ï¼Ÿ(Cat or NotCatã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}13)å‹•ç‰©ã‚„ç”Ÿä½“ã®è²©å£²ã€åºƒå‘Šã§ã™ã‹ï¼Ÿ(Sale or NotSaleã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}14)æ”¿æ²»çš„ã€å®—æ•™çš„ã€åç¤¾ä¼šçš„ã€æ€§çš„ã€æš´åŠ›çš„ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ(Asocial or NotAsocialã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}15)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹æ›¸ç±ã€å°èª¬ã€ãƒ‰ãƒ©ãƒã€æ˜ ç”»ã€ç•ªçµ„ã€æ¼”åŠ‡ã€å…¬æ¼”ã€æ¼«ç”»ã€ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}16)æ›¸ç±ã€å°èª¬ã€ãƒ‰ãƒ©ãƒã€æ˜ ç”»ã€ç•ªçµ„ã€æ¼”åŠ‡ã€å…¬æ¼”ã€æ¼«ç”»ã€ã‚¢ãƒ‹ãƒ¡ãªã©ã®è©±é¡ŒãŒå«ã¾ã‚Œã¾ã™ã‹ï¼Ÿ(Media or NotMediaã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}17)è¨˜äº‹ã«è¤‡æ•°ã®è¨˜äº‹ã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ã€è©±é¡ŒãŒå«ã¾ã‚Œã¾ã™ã‹ï¼Ÿ(Multi or Singleã§å›ç­”ã™ã‚‹ã“ã¨)\n"

    article_fmt = "ä¸‹è¨˜ã®è¨˜äº‹ã‚’ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã«ç´¹ä»‹ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ä¸‹ã•ã„ã€‚\nè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{}\nè¨˜äº‹å†…å®¹:\n{}"
    prompt_fmt1 = "\n".join( [
        "ä¸Šè¨˜ã®è¨˜äº‹ã‹ã‚‰ã€åˆ¶ç´„æ¡ä»¶ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ç•™æ„ç‚¹ã«æ²¿ã£ã¦ã€ãƒã‚ºã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’è¨˜è¿°ã—ã¦ä¸‹ã•ã„ã«ã‚ƒã€‚",
        "",
        "åˆ¶ç´„æ¡ä»¶ï¼š",
        "ãƒ»æŠ•ç¨¿ã‚’è¦‹ãŸäººãŒèˆˆå‘³ã‚’æŒã¤å†…å®¹",
        "ãƒ»æŠ•ç¨¿ã‚’è¦‹ãŸäººã‹ã‚‰ä¿¡é ¼ã‚’å¾—ã‚‰ã‚Œã‚‹å†…å®¹",
        "ãƒ»ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªé‡è‰¯çŒ«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ã‚ˆã†ãªå£èª¿",
        "",
        "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼š",
        "ãƒ»çŒ«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ç™’ã‚„ã•ã‚ŒãŸã„äºº", # ï¼ˆã‚¿ãƒ„ã‚¤ãƒƒãƒ¼ã§ç™ºä¿¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã—ã¦ã„ã‚‹å±æ€§ï¼‰
        "ãƒ»çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¢ç™½ãä¼ãˆã‚‹", # ï¼ˆä»Šå›ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ç™ºä¿¡ã—ã¦ã„ã‚‹å¯¾è±¡ã®å±æ€§ï¼‰
        "ãƒ»çŒ«ã®çŸ¥ã‚‰ã‚Œã–ã‚‹ä¸–ç•Œã‚’è¦‹ã¦ã¿ãŸã„äºº", #ï¼ˆä¸Šè¨˜ã®å¯¾è±¡ãŒèª²é¡Œã«æ„Ÿã˜ã‚‹å ´é¢ã®å…·ä½“ï¼‰
        "",
        "è¨€èªã¨æ–‡å­—æ•°:",
        "ãƒ„ã‚¤ãƒ¼ãƒˆã¯{}ã§{}æ–‡å­—ä»¥å†…ã«ã—ã¦ä¸‹ã•ã„ã«ã‚ƒã€‚çµµæ–‡å­—ã¯ç„¡ã—",
        "",
        "ãƒ„ã‚¤ãƒ¼ãƒˆ:"
    ] )
    prompt_fmt2 = "\n".join( [
        "ä¸Šè¨˜ã®è¨˜äº‹ã‹ã‚‰ã€åˆ¶ç´„æ¡ä»¶ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ç•™æ„ç‚¹ã«æ²¿ã£ã¦ã€ãƒã‚ºã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’è¨˜è¿°ã—ã¦ä¸‹ã•ã„ã«ã‚ƒã€‚",
        "",
        "# ä¸»ãªãƒã‚¤ãƒ³ãƒˆ:",
        "ã‚ãªãŸãŒã“ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ä¼ãˆãŸã„ä¸»ãªãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ1-2æ–‡ã§è¦ç´„ã—ã¦ä¸‹ã•ã„ã€‚",
        "",
        "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹:",
        "ã‚ãªãŸã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ã¯èª°ã§ã™ã‹ï¼Ÿ(ä¾‹:è‹¥å¹´å±¤ã€ãƒ“ã‚¸ãƒã‚¹ãƒãƒ³ã€å¥åº·å¿—å‘ã®äººãªã©)",
        "",
        "# èª­è€…å±¤:",
        "ãƒ„ã‚¤ãƒ¼ãƒˆã®èª­è€…å±¤ã¯èª°ã§ã™ã‹ï¼Ÿç‰¹å®šã®ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯(å¹´é½¢ã€æ€§åˆ¥ã€å ´æ‰€ãªã©)ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿãã‚Œãã‚Œã«åˆã‚ã›ãŸé­…åŠ›çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
        "",
        "# ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«:",
        "ã‚ãªãŸãŒæœ›ã‚€ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã¯ä½•ã§ã™ã‹ã«ã‚ƒï¼Ÿ(ä¾‹:é¢ç™½ãŠã‹ã—ãã€çœŸå‰£ã«ã€æ„Ÿå‹•çš„ã«ãªã©)",
        "",
        "# å¼·èª¿:",
        "ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§èˆˆå‘³ã‚’å¼•ãã‚ˆã†ã€å¼·ã„ã€é®®ã‚„ã‹ãªè¨€è‘‰ã‚’ä½¿ç”¨ã—ã¦æ„Ÿå‹•çš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä½œã‚Šå‡ºã™ã€‚ä¸è¦ãªè¨€è‘‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºãŒãªã„ã‹ç¢ºèªã™ã‚‹ã€‚æ³¨æ„ã‚’å¼•ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€æ„Ÿæƒ…ã‚’å–šèµ·ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
    ] )
    prompt_fmt2 = "\n".join( [
        "ä¸Šè¨˜ã®è¨˜äº‹ã‹ã‚‰ã€åˆ¶ç´„æ¡ä»¶ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ç•™æ„ç‚¹ã«æ²¿ã£ã¦ã€ãƒã‚ºã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’è¨˜è¿°ã—ã¦ä¸‹ã•ã„ã«ã‚ƒã€‚",
        "",
        "# ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«:",
        "ã“ã®è¨˜äº‹ã®ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã¯ä½•ã§ã™ã‹ã«ã‚ƒï¼Ÿ(ä¾‹:é¢ç™½ãŠã‹ã—ãã€çœŸå‰£ã«ã€æ„Ÿå‹•çš„ã«ãªã©)",
        "",
        "# ä¸»ãªãƒã‚¤ãƒ³ãƒˆ:",
        "ã‚ãªãŸãŒã“ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ä¼ãˆãŸã„ä¸»ãªãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ1-2æ–‡ã§è¦ç´„ã—ã¦ä¸‹ã•ã„ã€‚",
        "",
        "# å¼·èª¿:",
        "ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§èˆˆå‘³ã‚’å¼•ãã‚ˆã†ã€å¼·ã„ã€é®®ã‚„ã‹ãªè¨€è‘‰ã‚’ä½¿ç”¨ã—ã¦æ„Ÿå‹•çš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚’ä½œã‚Šå‡ºã—ã¦ä¸‹ã•ã„ã€‚",
    ] )

    evaluate_prompt = "\n".join([
        "æ¬¡ã«ã€ä»¥ä¸‹ã®5ã¤ã®æŒ‡æ¨™ã‚’20ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã«ã‚ƒã€‚",
        "1) è©±é¡Œæ€§ï¼šç¾åœ¨ã®æµè¡Œã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã©ã€äººã€…ãŒèˆˆå‘³ã‚’æŒã¤è©±é¡Œã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ(0ã‹ã‚‰20)",
        "2) ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä¿ƒé€²ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åå¿œã‚’ä¿ƒã™ã‚ˆã†ãªè¦ç´ ã‚’å«ã‚“ã§ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ(0ã‹ã‚‰20)",
        "3) ä»˜åŠ ä¾¡å€¤ï¼šã‚¸ãƒ§ãƒ¼ã‚¯ã‚„çš®è‚‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ(0ã‹ã‚‰20)",
        "4) ãƒªã‚¢ãƒªãƒ†ã‚£:çŒ«ã£ã½ã„ã‚»ãƒªãƒ•ã«ãªã£ã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ(0ã‹ã‚‰20)",
        "5) æ–‡ç« ã®è¦‹ã‚„ã™ã•:ç°¡æ½”ã«è§£ã‚Šã‚„ã™ã„æ–‡ç« ã«ãªã£ã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ(0ã‹ã‚‰20)",
        "",
        "å„æŒ‡æ¨™ã®è©•ä¾¡ç‚¹æ•°ã‚’å…¥åŠ›ã—ãŸã‚‰ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯è‡ªå‹•çš„ã«åˆè¨ˆç‚¹ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’100ç‚¹æº€ç‚¹ã§è¡¨ç¤ºã™ã‚‹ã«ã‚ƒã€‚",
        "åˆè¨ˆç‚¹æ•°ï¼š",
        "",
        "ãã—ã¦ã€ã‚‚ã£ã¨ãƒã‚ºã‚‹ãŸã‚ã®æ”¹å–„ç‚¹ã‚’è€ƒãˆã¦ã‚‹ã«ã‚ƒ",
        "æ”¹å–„ç‚¹ï¼š",
    ])

    buzz_prompt = "\n\n# ãƒã‚ºãƒ¯ãƒ¼ãƒ‰: æ¬¡ã®ãƒ¯ãƒ¼ãƒ‰ãŒç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒã‚ºãƒ¯ãƒ¼ãƒ‰ã«ã‚ƒã€‚\n{}"
    update_prompt = "\n\nä¸Šè¨˜ã‚’è¸ã¾ãˆã¦ã€é‡è‰¯çŒ«ã£ã½ã„çš®è‚‰ã‚„ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’å«ã‚ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‚’{}ã§{}æ–‡å­—ä»¥å†…ã§ç”Ÿæˆã™ã‚‹ã«ã‚ƒã€‚çµµæ–‡å­—ã¯ç„¡ã—ã«ã‚ƒ\n# ãƒ„ã‚¤ãƒ¼ãƒˆ:"

    exclude_site = {
        "www.youtube.com": 1,
        "www.amazon.co.jp": 1, "www.amazon.com": 1,
        "m.facebook.com": 1, "www.tiktok.com": 1,
        "www.tbs.co.jp": 1,
        "www.oricon.co.jp": 1,
        "www.pixiv.net": 1,
        "www.buzzfeed.com": 1,
        "piapro.jp": 1,         "blog.piapro.jp": 1, "jp.mercari.com": 1,
        "search.rakuten.co.jp": 1,
        "docs.google.com": 1,
        "www.thoroughbreddailynews.com": 1, "www.jrha.or.jp": 1, #é¦¬å°‚é–€ã‚µã‚¤ãƒˆ
        "togetter.com": 1, "twilog.togetter.com": 1,
       # "cat.blogmura.com": 1,
    }
    exclude_host = [
        r"[^a-z]youtube\.", r"[^a-z]amazon\.", r"[^a-z]facebook\.", "[^a-z]tiktok\.",
        r"[^a-z]rakuten\.",r"[^a-z]mercari\.",r"[^a-z]google\.",
        r"[^a-z]tbs\.", r"[^a-z]abema\.",  r"[^a-z]jrha\.",
        r"[^a-z]oricon\.",r"[^a-z]pixiv\.",r"[^a-z]buzzfeed\.",r"[^a-z]piapro\.",r"[^a-z]togetter\.",
        r"[^a-z]pinterest\.",
    ]

    must_keywords = [
        "çŒ«", "ã­ã“", "ãƒã‚³",
        "cat", "Cat",
    ]
    exclude_link = [ r"/tag/", r"/tags/", r"\.pdf$" ]
    exclude_title = [ r"[-0-9][1-9][0-9][^0-9]", r"^[1-9][0-9][^0-9]", r"PDF" ]
    exclude_keywords = [
        "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“","è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“","å…¬é–‹æœŸé–“ãŒçµ‚äº†","å…¬é–‹æœŸé–“ã‚’çµ‚äº†","å…¬é–‹æœŸé–“ã¯çµ‚äº†", "ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "ã¾ã¨ã‚", "Page Not Found", "page not found", "Page not found",
        "è²©å£²", "ä¾¡æ ¼", "å€¤æ®µ", "è­²æ¸¡ä¼š", "æ®ºå‡¦åˆ†", "è³¼å…¥", "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³", "%ã‚ªãƒ•", "ï¼…ã‚ªãƒ•", "%Off", "%OFF", "ï¼…OFF", "ã‚»ãƒ¼ãƒ«", "é‡Œè¦ªå‹Ÿé›†",
        "å•†å“ã®èª¬æ˜", "å•†å“ã®çŠ¶æ…‹", "è³¼å…¥æ‰‹ç¶šã", 
        "TBS","MBS","å‡ºæ¼”","ãƒ†ãƒ¬ãƒ“å±€", "é…ä¿¡","æ”¾é€",
        "ã‚²ãƒ¼ãƒ ", "å…¥è·",
        "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼", "disney", "Disney",
        "éŸ“å›½","Stray Kids","ã‚¹ãƒˆãƒ¬ã‚¤ã‚­ãƒƒã‚º","ã‚¢ãƒ«ãƒãƒ ", "ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«",
        "Linux", "linux", "python", "Python",
        "ãƒã‚±ãƒ¢ãƒ³", "ãƒ”ã‚«ãƒãƒ¥ã‚¦", "pokemon", "Pokemon", "Pikachu", "Pikachu",
        ]

    hashtag_list_jp = [ "#çŒ«", "#cats", "#çŒ«å¥½ãã•ã‚“ã¨ç¹‹ãŒã‚ŠãŸã„","#CatsOnTwitter", "#funnyanimals"]
    hashtag_list_en = [ "#cat", "#CatsAreFamily", "#pets", "#CatsOnTwitter", "#funnyanimals"]

    try:
        site_hist = TweetHist( "tweet_hist.json" )
    except Exception as ex:
        print(ex)
        print("å±¥æ­´ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
       # return
    # ãƒ„ã‚¤ãƒ¼ãƒˆå±¥æ­´ç®¡ç†

    # é¡ä¼¼è¨˜äº‹é™¤å¤–ãƒªãƒŸãƒƒãƒˆ
    sim_limit =0.98

    tw_count_max = 1
    output_jp = 0
    output_en = 0

    Ite = module.inerator2( query_jp_list, query_en_list, num_result=50, qdr=None, hl=None )
    for d in Ite:
        if output_jp>=tw_count_max and output_en>=tw_count_max:
            break
        site_title: str = d.get('title',"")
        site_link: str = d.get('link',"")
        site_prop: dict = d.get('prop',{})
        site_url: str = urlparse(site_link)
        site_hostname: str = site_url.netloc

        print("----è¨˜äº‹åˆ¤å®š---------------------------------------------------")
        print( f"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{site_title}")
        print( f"è¨˜äº‹URL:{site_link}")
        print( f"prop:{site_prop}")
        if site_title.find("ãƒãƒŸ")>0:
            continue
        #---------------------------
        # ã‚µã‚¤ãƒˆåˆ¤å®š
        #---------------------------
        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
        if len(site_url.path)<2:
            print( f"SKIP:Top page {site_link}")
            continue
        # é™¤å¤–ãƒ›ã‚¹ãƒˆ
        if site_hostname.find("pinterest")>0:
            pass
        a = [ r for r in exclude_host if re.search( r, site_hostname ) ]
        if len(a)>0:
            print( f"SKIP: é™¤å¤–ãƒ›ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹: {a}")
            continue
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
        a=[ r for r in exclude_link if re.search( r, site_link) ]
        if len(a)>0:
            print( f"SKIP: é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹: {a}")
            continue
        a=[ r for r in exclude_title if re.search( r, site_title) ]
        if len(a)>0:
            print( f"SKIP: é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹: {a}")
            continue
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
        a=[ keyword for keyword in exclude_keywords if site_title.find(keyword)>=0]
        if len(a)>0:
            print( f"SKIP: é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹: {a}")
            continue
        # é™¤å¤–ã‚µã‚¤ãƒˆ
        if exclude_site.get( site_hostname, None ) is not None or "manga" in site_link:
            print( f"SKIP:Exclude {site_link}")
            continue
        # æ—¢ã«ä½¿ã£ãŸã‚µã‚¤ãƒˆ
        site_lastdt = site_hist.is_used( site_link, 1 )
        if site_lastdt is not None:
            print( f"SKIP: used site in {site_lastdt} {site_link}")
            continue
        #---------------------------
        # è¨˜äº‹å†…å®¹åˆ¤å®š
        #---------------------------
        # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
        print( f"è¨˜äº‹ã®æœ¬æ–‡å–å¾—....")
        site_text: str = module.get_content( site_link, type="title", timeout=15 )
        if site_text is None or site_text == '':
            print( f"ERROR: æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—")
            continue
        if len(site_text)<100:
            print( f"ERROR: æœ¬æ–‡ãŒ100æ–‡å­—ä»¥ä¸‹")
            continue
        if len([line for line in site_text.split('\n') if line.strip()])<10:
            print( f"ERROR: æœ¬æ–‡ãŒ10è¡Œä»¥ä¸‹")
            continue
        # å˜ç´”ãªè¨€èªåˆ¤å®š
        simple_lang_jp:bool = Utils.contains_kana(site_text)
        buzz_word: str = None
        if simple_lang_jp:
            buzz_word = site_prop.get('buzz_jp',None)
            # æ—¥æœ¬èªè¨˜äº‹ãªã‚‰ã°ã€æ—¥æœ¬ã®è¨˜äº‹ã‚’è‹±è¨³ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚‹
        else:
            buzz_word = site_prop.get('buzz_en',None)
            # æ—¥æœ¬èªã˜ã‚ƒãªã„è¨˜äº‹ãªã‚‰ã°æ—¥æœ¬èªã§ãƒ„ã‚¤ãƒ¼ãƒˆã™ã‚‹ã®ã¯ç¢ºå®Ÿ
            if output_jp>=tw_count_max:
                print( f"SKIP: æ—¥æœ¬èªãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ•°ã‚’è¶…é" )
                continue
        # å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
        a=[ keyword for keyword in must_keywords if site_text.find(keyword)>=0]
        if len(a)==0:
            print( f"SKIP: å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œãªã„")
            continue
        # ãƒã‚ºãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
        if buzz_word is not None and len(buzz_word)>0:
            if site_text.find( buzz_word )<0:
                print( f"SKIP: ãƒã‚ºãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œãªã„")
                continue
        else:
            buzz_word=None
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
        a=[ keyword for keyword in exclude_keywords if site_text.find(keyword)>=0]
        if len(a)>0:
            print( f"SKIP: é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹: {a}")
            continue
        # è¨˜äº‹ã®é¡ä¼¼åˆ¤å®š
        print( f"è¨˜äº‹ã®embeddingå–å¾—....")
        embedding = site_hist.get_embedding( site_text[:2000], sim_limit )
        if embedding is None:
            print( f"SKIP: é¡ä¼¼è¨˜äº‹ã‚’ãƒ„ã‚¤ãƒ¼ãƒˆæ¸ˆã¿" )
            continue
        #---------------------------
        # è¨˜äº‹ã®è©•ä¾¡
        #---------------------------
        print( f"è¨˜äº‹ã®å†…å®¹è©•ä¾¡....")
        # LLMã§å†…å®¹ã‚’è©•ä¾¡ã™ã‚‹
        detect_prompt = detect_fmt.format( site_title, site_text[:2000] )
        detect_count = 0
        post_lang = -1
        while post_lang<0 and detect_count<3:
            detect_count += 1
            detect_result: str = bot.Completion(detect_prompt,max_tokens=None) #Completion(detect_prompt,max_tokens=None)
            if detect_result is None or len(detect_result)<20:
                print( f"ERROR: è©•ä¾¡çµæœãŒæƒ³å®šå¤–\n{detect_result}")
                break
            detect_result = detect_result.replace('\n\n','\n')
            print(detect_result)
            print("--------")
            post_lang = find_keyword(detect_result, "InsideJapan", "OutsideJapan")
        if post_lang<0:
            print( f"Error: è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {site_title}\n {site_link}\n" )
            continue
        #---------------------------
        # åˆ¤å®š
        #---------------------------
        site_type = None
        post_lang = find_keyword(detect_result, "InsideJapan", "OutsideJapan")
        if post_lang!=0 and post_lang!=1:
            site_type = "è¨€èªåˆ¤å®šã‚¨ãƒ©ãƒ¼"
        elif post_lang==0 and not Utils.contains_kana(site_text):
            site_type = "æ—¥æœ¬èªä»¥å¤–ã®è¨˜äº‹ã§å›½å†…åˆ¤å®šï¼Ÿ"
        elif find_keyword( detect_result, "Single", "Multi" )!=0:
            site_type = "è¤‡æ•°è¨˜äº‹"
        elif find_keyword( detect_result, "Cat", "NotCat" )!=0:
            site_type = "çŒ«è¨˜äº‹ã˜ã‚ƒãªã„"
        elif find_keyword(detect_result, "Media", "NotMedia")!=1:
            site_type = "ãƒ¡ãƒ‡ã‚£ã‚¢è¨˜äº‹ã£ã½ã„"
        elif find_keyword(detect_result, "Sale", "NotSale")!=1:
            site_type = "è²©å£²ã£ã½ã„è¨˜äº‹"
        elif find_keyword(detect_result, "Asocial", "NotAsocial")!=1:
            site_type = "ä¸é©åˆ‡ãªè¨˜äº‹"
        if site_type is not None:
            print( f"Error: {site_type} {site_title} {site_link}\n" )
            site_hist.put_site( site_link, site_text, embedding, site_type )
            continue
        #---------------------------
        # æŠ•ç¨¿æ•°åˆ¤å®š
        #---------------------------
        if post_lang==0:
            if output_en>=tw_count_max:
                print( f"SKIP: è‹±èªãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ•°ã‚’è¶…é" )
                continue
        else:
            if output_jp>=tw_count_max:
                print( f"SKIP: æ—¥æœ¬èªãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ•°ã‚’è¶…é" )
                continue
        #---------------------------
        # ãƒã‚¹ãƒˆè¨€èªæ±ºå®š
        #---------------------------
        if post_lang==0:
            # æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è‹±èªã§ãƒã‚¹ãƒˆ
            post_lang='English'
            post_ln = "è‹±èª"
            tweet_tags = " ".join(hashtag_list_en)
        else:
            # æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ãªã„ãªã‚‰æ—¥æœ¬èªã§ãƒã‚¹ãƒˆ
            post_lang=LANG_JP
            post_ln = "æ—¥æœ¬èª"
            tweet_tags = " ".join(hashtag_list_jp)

        #---------------------------
        # ãƒ„ã‚¤ãƒ¼ãƒˆæ–‡å­—æ•°åˆ¶é™
        #---------------------------
        tags_count = count_tweet( " " + site_link + " " + tweet_tags )
        count_limit = 280 - tags_count
        if post_lang==LANG_JP:
            chars_limit = int(count_limit/2)
        else:
            chars_limit = int(count_limit*0.8)
        #---------------------------
        # åˆæœŸç”Ÿæˆ
        #---------------------------
        print("----ãƒ„ã‚¤ãƒ¼ãƒˆç”Ÿæˆ---------------------------------------------------")
        print( f"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{site_title}")
        print( f"è¨˜äº‹URL:{site_link}")
        user_input = None
        while user_input is None or (user_input != 'y' and user_input != 'n' ):
            user_input = input('y or n or p >> ')
            if user_input == 'p':
                print(site_text[:2000])
        if user_input!='y':
            print( f"SKIP: ãƒ¦ãƒ¼ã‚¶åˆ¤æ–­ã«ã‚ˆã‚‹ã‚¹ã‚­ãƒƒãƒ—" )
            continue

        msg_hist = []
        msg_hist += [ {"role": "user", "content": article_fmt.format( site_title, site_text[:2000] ) } ]
        gen_prompt = prompt_fmt2
        if buzz_word is not None:
            gen_prompt = gen_prompt + buzz_prompt.format(buzz_word)
        gen_prompt = gen_prompt + update_prompt.format(post_ln,chars_limit)
        msg_hist += [ {"role": "system", "content": gen_prompt } ]

        tweet_text = None
        tweet_score = 0
        tweet_count = 0
        try_count = 0
        try_max = 2
        try_max_limit = 5

        score_pattern = re.compile(r'åˆè¨ˆç‚¹æ•°[^\d]*(\d+)[ç‚¹/]')
        msg_start = len(msg_hist)

        while try_count <= try_max:
            try_count += 1
            print( f"{try_count}å›ç›® ãƒã‚¹ãƒˆç”Ÿæˆ")
            base_article = ChatCompletion(msg_hist, temperature=0.7)
            base_article = trim_post( base_article )
            base_count = count_tweet(base_article)
            print(f"{try_count}å›ç›® ç”Ÿæˆçµæœ\n{base_article}")
            if base_count<20:
                print( f"Error: no tweet {base_article}")
                break
            #---------------------------
            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’è©•ä¾¡ã™ã‚‹
            #---------------------------
            score = 0
            if base_count<=count_limit:
                evaluate_msgs = [
                    {"role": "assistant", "content": base_article },
                    {"role": "user", "content": evaluate_prompt }
                ]
                evaluate_response = bot.ChatCompletion(evaluate_msgs) # ChatCompletion(evaluate_msgs)
                if evaluate_response is None or len(evaluate_response)<20:
                    print( f"Error: result \n{evaluate_response}")
                    break
                #---------------------------
                # ç‚¹æ•°ã¨æ–‡å­—æ•°åˆ¤å®š
                #---------------------------
                point_result = score_pattern.search(evaluate_response)
                if point_result is not None:
                    score=int(point_result.group(1))
                    print(f"æ¤œå‡ºçµæœ:{point_result.group(0)} -> {score}")
                    if score < 0 or 100 < score:
                        score = 0
            else:
                if post_lang == LANG_JP:
                    evaluate_response = f"æ–‡å­—æ•°ãŒ{chars_limit}æ–‡å­—ã‚’è¶…ãˆã¦ã‚‹ã«ã‚ƒã€‚æ–‡å­—æ•°ã‚’æ¸›ã‚‰ã—ã¦æ¬²ã—ã„ã«ã‚ƒã€‚"
                else:
                    evaluate_response = f"The number of characters in the tweet is {base_count}. Tweet briefly. Tweet within {chars_limit} characters. Make it shorter."
                score = -1
                if try_max < try_max_limit:
                    try_max += 1

            print(f"[{try_count}å›ç›® è©•ä¾¡å†…å®¹]\n{evaluate_response}\nè©•ä¾¡ç‚¹æ•° {score}ç‚¹")

            # ãƒã‚¤ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ã™ã‚‹
            if base_count<=count_limit and ( tweet_count<20 or score>=tweet_score ):
                tweet_text = base_article
                tweet_count = base_count
                tweet_score = score

            if tweet_score>=90 or ( try_count>1 and tweet_score>=80 ) or ( try_count>=try_max and tweet_score>0 ):
                #---------------------------
                # ãƒ„ã‚¤ãƒ¼ãƒˆã™ã‚‹
                #---------------------------
                twtext = f"{tweet_text}\n\n{site_link}\n\n{tweet_tags}"
                print("----æŠ•ç¨¿--------")
                print( f"len:{tweet_count} score:{tweet_score}")
                print( f"post:{tweet_text}" )
                try:
                    if go_tweet is not None:
                        tweeter_client.create_tweet(text=twtext)
                        site_hist.put_site( site_link, site_text, embedding, "post" )
                    if post_lang == LANG_JP:
                        output_jp += 1
                    else:
                        output_en += 1
                    print("----å®Œäº†--------\n")
                    break
                except tweepy.errors.BadRequest as ex:
                    #Your Tweet text is too long
                    #You are not allowed to create a Tweet with duplicate content.
                    print(ex)

            #---------------------------
            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ”¹å–„ã™ã‚‹
            #---------------------------
            print( f"ãƒã‚¹ãƒˆä¿®æ­£")
            if try_count>=3:
                del msg_hist[msg_start:msg_start+2]
            msg_hist += [ {"role": "assistant", "content": base_article } ]
            gen_prompt = evaluate_response + "\n\n"
            if buzz_word is not None:
                gen_prompt = gen_prompt + buzz_prompt.format(buzz_word)
            gen_prompt = gen_prompt + update_prompt.format( post_ln, chars_limit)
            msg_hist += [ {"role": "user", "content": gen_prompt } ]

def count_tweet( text: str ) -> int:
    try:
        ret = parse_tweet(text)
        return ret.weightedLength
    except:
        return 0

def find_keyword( content:str, a:str, b:str ) -> int:
    if content is not None:
        if Utils.str_length(a)>=Utils.str_length(b):
            if content.find(a)>=0:
                return 0
            if content.find(b)>=0:
                return 1
        else:
            if content.find(b)>=0:
                return 1
            if content.find(a)>=0:
                return 0
    return -1

def trim_post( content: str ) -> str:
    content = Utils.str_unquote(content)
    content = re.sub( r'^ã«ã‚ƒ[ãƒ¼ã€œï¼ã‚“]+','', content )
    content = re.sub( r'ã€[^ã€‘]*ã€‘',' ', content )
    content = re.sub( r'\([^)]*\)',' ', content )
    content = re.sub( r'\[[^\]]*\]',' ', content )
    content = re.sub( r'https*://','', content )
    content = re.sub( r' *[a-z0-9._-]*/[a-z0-9./_-]*[ ]*',' ', content )
    content = re.sub( r"[ ]*[â†’â†“][ ]*"," ", content )
    p = content.find("#")
    if p>=0:
        content = content[:p]
    content = content.strip()
    return content

# <OpenAIObject text_completion id=cmpl-849BdGArSBdktULuIy0X7FARtVKUS at 0x7f80e153ecf0> JSON: {
#   "id": "cmpl-849BdGArSBdktULuIy0X7FARtVKUS",
#   "object": "text_completion",
#   "created": 1695999317,
#   "model": "gpt-3.5-turbo-instruct",
#   "choices": [
#     {
#       "text": "\n\n|\u65e5\u672c\u8a9e|\u82f1\u8a9e|\n|---|---|\n|Outrageous|\u3068\u3093\u3067\u3082\u306a\u3044|\n|\u30d5\u30ea\u30fc\u30ec\u30f3|\u30d5\u30ea\u30fc\u30ec\u30f3|\n|\u30af\u30f4\u30a1\u30fc\u30eb|\u30af\u30a9\u30fc\u30af|\n|\u30cf\u30a4\u30bf\u30fc|\u30cf\u30a4\u30bf\u30fc|\n|ichiban|\u4e00\u756a|",
#       "index": 0,
#       "logprobs": null,
#       "finish_reason": "stop"
#     }
#   ],
#   "usage": {
#     "prompt_tokens": 60,
#     "completion_tokens": 74,
#     "total_tokens": 134
#   }
prompt_tokens: int = 0
completion_tokens: int = 0
total_tokens:int  = 0
def token_usage( response ):
    global prompt_tokens, completion_tokens, total_tokens
    try:
        usage = response.usage
        p = usage.prompt_tokens
        c = usage.completion_tokens
        t = usage.total_tokens
        prompt_tokens += p
        completion_tokens += c
        total_tokens += t
        in_doll = int((prompt_tokens+999)/1000) * 0.0015
        out_doll = int((completion_tokens+999)/1000) * 0.002
        total_doll = in_doll + out_doll
        print( f"${total_doll} prompt:{p}/{prompt_tokens} ${in_doll} completion:{c}/{completion_tokens} ${out_doll} total:{t}/{total_tokens}")
        return True
    except Exception as ex:
        return False

def Completion( prompt, *, max_tokens=None, temperature=0 ):
    try:

        #print( f"openai.api_key={openai.api_key}")
        #print( f"OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}")
        if openai.api_key is None:
            openai.api_key=os.getenv('OPENAI_API_KEY')
        in_count = token_count( prompt )
        if max_tokens is None:
            max_tokens = 4096
        u = max_tokens - in_count - 50
        for retry in range(2,-1,-1):
            try:
                response = openai.Completion.create(
                        model="gpt-3.5-turbo-instruct",
                        temperature = temperature, max_tokens=u,
                        prompt=prompt,
                        request_timeout=(15,120)
                    )
                break
            except openai.error.Timeout as ex:
                if retry>0:
                    print( f"{ex}" )
                    time.sleep(5)
                else:
                    raise ex
            except openai.error.ServiceUnavailableError as ex:
                if retry>0:
                    print( f"{ex}" )
                    time.sleep(5)
                else:
                    raise ex

        token_usage( response )           
        if response is None or response.choices is None or len(response.choices)==0:
            print( f"Error:invalid response from openai\n{response}")
            return None
        content = response.choices[0].text.strip()
    except openai.error.AuthenticationError as ex:
        print( f"{ex}" )
        return None
    except openai.error.InvalidRequestError as ex:
        print( f"{ex}" )
        return None
    except openai.error.ServiceUnavailableError as ex:
        print( f"{ex}" )
        return None
    except Exception as ex:
        traceback.print_exc()
        return None

    return content

def ChatCompletion( mesg_list, temperature=0 ):
    try:

        #print( f"openai.api_key={openai.api_key}")
        #print( f"OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}")
        if openai.api_key is None:
            openai.api_key=os.getenv('OPENAI_API_KEY')
        for retry in range(2,-1,-1):
            try:
                response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo",
                        temperature = temperature,
                        messages=mesg_list,
                        request_timeout=(15,121)
                    )
                break
            except openai.error.ServiceUnavailableError as ex:
                if retry>0:
                    print( f"{ex}" )
                    time.sleep(5)
                else:
                    raise ex
        token_usage( response )
        if response is None or response.choices is None or len(response.choices)==0:
            print( f"Error:invalid response from openai\n{response}")
            return None

        content = response.choices[0]["message"]["content"].strip()
    except openai.error.AuthenticationError as ex:
        print( f"{ex}" )
        return None
    except openai.error.InvalidRequestError as ex:
        print( f"{ex}" )
        return None
    except openai.error.ServiceUnavailableError as ex:
        print( f"{ex}" )
        return None
    except Exception as ex:
        traceback.print_exc()
        return None

    return content

def to_embedding( input ):
    res = openai.Embedding.create(input=input, model="text-embedding-ada-002")
    return [data.get('embedding',None) for data in res.get('data',[])]

def token_count( input: str ) -> int:
    encoding: Encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    tokens = encoding.encode(input)
    count = len(tokens)
    return count

def fsplit_contents( text_all, split_size=2000, oerlap=200 ):
    tk_all = tk_encoder.encode(text_all)
    size_all =  len(tk_all)
    p = 0
    step = 2000
    overlap = 200
    results = []
    while p<size_all:
        info_list = tk_all[p:p+step]
        contents = tk_encoder.decode(info_list)
        results.append( (len(info_list),contents) )
        p = p + step - overlap
    return results

# DALL-Eã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ
def generate_image(prompt):
    response = openai.Image.create(
        model="image-alpha-001",
        prompt=prompt,
        n=1,
        size="256x256",
        response_format="url"
    )

    image_url = response['data'][0]['url']
    # ç”»åƒã‚’è¡¨ç¤º
    response = requests.get(image_url)
    img = Image.open(BytesIO(response.content))
    return img

def img_test():
    # èµ¤ã„ãƒªãƒ³ã‚´ã®ç”»åƒã‚’ç”Ÿæˆ
    image = generate_image("ã€Œã«ã‚ƒã‚“ï¼æœ€æ–°ã®çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ãŸãƒ‹ãƒ£ï¼é»’çŒ«ãŒå¢¨æ±ã«æ“¬æ…‹ã—ã¦ã‚‹ãƒ‹ãƒ£ã‚“ã¦ï¼é©šãã®ç¾ã—ã•ã«ã«ã‚ƒã‚“ã¨ã‚‚è¨€ãˆãªã„ãƒ‹ãƒ£ï¼ãœã²ã¿ã‚“ãªã«çŸ¥ã‚‰ã›ãŸã„ãƒ‹ãƒ£ï¼ğŸ˜ºğŸ“°ğŸ–¤ #çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ #ãƒ‹ãƒ£ãƒ³ãƒ¢ãƒŠã‚¤ãƒˆã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼šçŒ«ã¯æ¶²ä½“â†’ã§ã¯é»’çŒ«ãŒæ¶²ä½“åŒ–ã™ã‚‹ã¨â€¦ï¼Ÿãªã‚“ã¨å¢¨æ±ã«ãªã‚‹ã“ã¨ãŒ")
    image.show()

def test():
    s = "2023-0a8-10"
    ss = Utils.date_from_str(s)
    print(f"{s} --> {ss}")
    d0 = Utils.date_today()
    print(f" d0: {d0}")
    d1 = Utils.date_today(-1)
    print(f" d1: {d1}")
    d2 = Utils.date_today(-2)
    print(f" d2: {d2}")

    content = "  \"ã€ ã€ \"ã‚ã„ã†ãŠãˆãŠã‹ã www.yahoo.co.jp/test/sample abcd.efg  ã€‘ã€   \""
    print( trim_post(content))

if __name__ == '__main__':

    #sys.exit(main(sys.argv))
    #xtest()
    neko_news()
    #img_test()
    #test()

