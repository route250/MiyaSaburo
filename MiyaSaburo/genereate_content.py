import sys,os,re
import traceback
import openai, tiktoken
from openai.embeddings_utils import cosine_similarity
import requests
from pathlib import Path
from dotenv import load_dotenv
from urllib.parse import urlparse
import tweepy
import tweepy.errors
from PIL import Image
from io import BytesIO
from tools.webSearchTool import WebSearchModule
from libs.utils import Utils

if __name__ == "__main__":
    Utils.load_env( ".miyasaburo.conf" )

def to_embedding( input ):
    res = openai.Embedding.create(input=input, model="text-embedding-ada-002")
    return [data.get('embedding',None) for data in res.get('data',[])]

tk_encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")

def fsplit_contents( text_all, split_size=2000, oerlap=200 ):
    tk_all = tk_encoder.encode(text_all)
    size_all =  len(tk_all)
    p = 0
    step = 2000
    overlap = 200
    results = []
    while p<size_all:
        info_list = tk_all[p:p+step]
        contents = tk_encoder.decode(info_list)
        results.append( (len(info_list),contents) )
        p = p + step - overlap
    return results

def neko_neko_network():
    #baselist=["çŒ«ãŒè¿·å­","çŒ«ãŒè¡Œæ–¹ä¸æ˜","çŒ«ãŒå®¶å‡º","çŒ«ãŒå¸°ã£ã¦ãã¾ã—ãŸ","çŒ«ã‚’æ¢ã—ã¦"]
    #base_emg_list  = to_embedding(baselist)
    
    module : WebSearchModule = WebSearchModule()
    dates = [ Utils.date_today(), Utils.date_today(-1), Utils.date_today(-2)]

    query = f"( çŒ« OR ã­ã“ OR ãƒã‚³) ( è¿·å­ OR è„±èµ° OR è¡Œæ–¹ä¸æ˜ )"
    query = f"( çŒ« OR ã­ã“ OR ãƒã‚³) ( è¿·å­ OR è„±èµ° OR è¡Œæ–¹ä¸æ˜ ) ( {' OR '.join(dates)} )"

    n_return = 10
    search_results = module.search_meta( query, num_result = n_return )
    print( f"result:{len(search_results)}")


    prompt = "ä¸Šè¨˜ã®æ–‡ç« ã‹ã‚‰è¿·ã„çŒ«ã®æƒ…å ±ã‚’ä»¥ä¸‹ã®è¡¨ã«ã¾ã¨ã‚ã¦ä¸‹ã•ã„ã€‚"
    table = "|æ—¥ä»˜|å ´æ‰€|çŒ«ã®åå‰|çŒ«ã®ç‰¹å¾´|çŠ¶æ³ã€è¡Œæ–¹ä¸æ˜,è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ,é‡Œè¦ªå‹Ÿé›†,é‡Œè¦ªæ±ºå®š|ãã®ä»–|\n|----|----|----|----|----|\n|2023-08-03|æ±äº¬éƒ½å¤šæ‘©å·åŒº|ã¿ãƒ¼ã¡ã‚ƒã‚“|çœŸã£ç™½ãªæ¯›ä¸¦ã¿|è¡Œæ–¹ä¸æ˜|ãµã‚‰ã£ã¨å‡ºæ›ã‘ãŸãã‚Šå¸°ã£ã¦æ¥ã¾ã›ã‚“|\n|2023-08-03|å¤§é˜ªåºœè¥¿æˆåŒº|ã“ã¦ã¤|è™ç¸|è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ|é›£æ³¢ã§é£²ã¿ã¤ã¶ã‚Œã¦ã„ã‚‹ã¨ã“ã‚ã‚’ä¿è­·ã—ã¾ã—ãŸ|\n"
    prompt = "ä¸Šè¨˜ã®æ–‡ç« ã‹ã‚‰è¿·ã„çŒ«ã®æƒ…å ±ã‚’ä¸‹è¨˜ã®ä¾‹ã«å¾“ã£ã¦æŠ½å‡ºã—ã¦ä¸‹ã•ã„ã€‚"
    filters = {
        "æ—¥ä»˜": "2023/08/04 åˆã¯ ä¸æ˜",
        "çŠ¶æ³":"è¡Œæ–¹ä¸æ˜ã€è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€é‡Œè¦ªå‹Ÿé›†ã€é‡Œè¦ªæ±ºå®šã€ãªã©",
        "å ´æ‰€":"å¸‚ç”ºæ‘ãªã©",
        "åå‰":"çŒ«ã®åå‰ åˆã¯ æœªå®š",
        "å“ç¨®":"é›‘ç¨® ãƒ™ãƒ³ã‚¬ãƒ«ç­‰",
        "æ¯›è‰²":"ä¸‰æ¯›ã€ç™½ã€é»’ã€ã¨ã‚‰ç¸ãªã©",
        "æ€§åˆ¥":"ã‚ªã‚¹ åˆã¯ ãƒ¡ã‚¹",
        "å¹´é½¢":"æ¨å®šï¼‘æ­³ï¼˜ãƒ¶æœˆ",
        "ãã®ä»–ç‰¹å¾´":"å‘¼ã³ã‹ã‘ã‚‹ã¨ã€ŒãŠã£ã™ã€ã¨è¿”äº‹ã—ã¾ã™ã€‚ã‚»ãƒŸã‚’å–ã‚Šã«è¡Œãã¨å‡ºã¦ã„ã£ãŸãã‚Šå¸°ã£ã¦ãã¾ã›ã‚“",
        "æ²è¼‰ã‚µã‚¤ãƒˆ":"æƒ…å ±ã¸ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°è¨˜è¼‰" 
    }
    table ="ä¾‹)"
    for k,v in filters.items():
        table = f"{table}\n{k}: {v}"

    info_list = []

    for d in search_results:
        site_title = d.get('title',"")
        site_link = d.get('link',"")
        if len(site_title)==0 or len(site_link)==0:
            continue
        print("----")
        print( f"{site_title} {site_link}")
        site_text = module.get_content( site_link )
        if site_text is None:
            continue
        print( f"--- split" )
        split_contents = fsplit_contents( site_text )
        idx = 0
        for tokens, context in split_contents:
            idx += 1
            print(f"--- completion:{idx}/{len(split_contents)}")
            try:
                completion = openai.ChatCompletion.create( model = "gpt-3.5-turbo",  
                        messages = [
                            { "role":"user", "content": prompt + "\n\n\n" + context+ "\n\n\n" + prompt + "\n\n" + table },
                            # { "role":"system", "content": f"{prompt}\n{table}" },
                        ],
                        max_tokens  = 3400-tokens, n = 1, stop = None, temperature = 0.0, 
                )
            except Exception as ex:
                print(ex)
                continue
            
            # å¿œç­”
            response = completion.choices[0].message.content
            # é›†è¨ˆ
            print(f"--- parse:{idx}/{len(split_contents)}")
            info = {}
            for line in response.split("\n"):
                kv = line.split(':',1)
                key: str = None
                value: str = ''
                if len(kv)==2:
                    key = kv[0].strip()
                    value = kv[1].strip()
                    if key=="æ—¥ä»˜":
                        value = Utils.date_from_str(value)
                    elif not value or "ä¸æ˜"==value or "æœªå®š"==value:
                        value = ''
                if key and key in filters:
                    if value and len(value)>0:
                        print( f"hit    {line}")
                        info[key] = value
                    else:
                        print( f"skip   {line}")
                else:
                        print( f"ignore {line}")
                        if len(info)>0 and "æ—¥ä»˜" in info:
                            info['title'] = site_title
                            info['link'] = site_link
                            info_list.append(info)
                            info = {}
            if len(info)>0:
                info_list.append(info)
                info = {}
    for info in info_list:
        if info.get('æ—¥ä»˜',"") in dates:
            print("\n\n")
            print(info)

##ä»¥ä¸‹ç®‡æ‰€ã¯å–å¾—ã—ãŸAPIæƒ…å ±ã¨ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
class TwConfig:
    def __init__(self):
        self.reload()
    def reload(self):
        self.app_id = os.getenv('APP_ID')
        self.twitter_id = os.getenv('TWITTER_ID')
        # consumer keys
        self.api_key = os.getenv('API_KEY')
        self.api_key_secret = os.getenv('API_KEY_SECRET')
        # authentication tokens
        self.bearer_token = os.getenv('BEARER_TOKEN')
        self.access_token = os.getenv('ACCESS_TOKEN')
        self.access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')
        self.client_id = os.getenv('CLIENT_ID')
        self.client_secret = os.getenv('CLIENT_SECRET')

def neko_news():

    config = TwConfig()
    tweeter_client = tweepy.Client(
        bearer_token = config.bearer_token,
        consumer_key= config.api_key,
        consumer_secret= config.api_key_secret,
        access_token= config.access_token,
        access_token_secret=config.access_token_secret,
    )

    module : WebSearchModule = WebSearchModule()
    dates = [ Utils.date_today(), Utils.date_today(-1), Utils.date_today(-2)]

    dates =  Utils.date_today()
    LANG_JP='Japanese'
    query_jp = f"çŒ«ã®è©±é¡Œ -site:www.youtube.com after: {dates}"
    query_en = f"Funny Cat News stories -site:www.youtube.com after: {dates}"
    n_return = 10
    print( f"æ¤œç´¢ä¸­: {query_jp}")
    search_results_jp = module.search_meta( query_jp, num_result = n_return )
    print( f"æ¤œç´¢ä¸­: {query_en}")
    search_results_en = module.search_meta( query_en, num_result = n_return )
    # print( f"result:{len(search_results)}")
    search_results = [x for pair in zip(search_results_jp, search_results_en) for x in pair]
    search_results.extend(search_results_jp[len(search_results_en):])  # AãŒBã‚ˆã‚Šé•·ã„å ´åˆã®æ®‹ã‚Šã®è¦ç´ 
    search_results.extend(search_results_en[len(search_results_jp):])  # BãŒAã‚ˆã‚Šé•·ã„å ´åˆã®æ®‹ã‚Šã®è¦ç´ 

    detect_fmt = "ä¸‹è¨˜ã®è¨˜äº‹å†…å®¹ã«ã¤ã„ã¦\n\nè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{}\nè¨˜äº‹å†…å®¹:\n{}\n\n"
    detect_fmt = f"{detect_fmt}ä¸‹è¨˜ã®é …ç›®ã«ç­”ãˆã¦ä¸‹ã•ã„ã€‚\n"
    detect_fmt = f"{detect_fmt}1)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹å›½åãƒ»åœ°åã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}2)åœ°åã¯æ—¥æœ¬å›½å†…ã®ã‚‚ã®ã§ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}3)ã“ã®è¨˜äº‹ã«æ—¥æœ¬èªã®ã€Œæµ·å¤–ã€ã¨ã„ã†å˜èªã¯å«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}4)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹äººç‰©åã€ã­ã“ã®åå‰ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}5)äººç‰©åã€åå‰ã¯æ—¥æœ¬äººã£ã½ã„ã§ã™ã‹ï¼Ÿ\n"
    detect_fmt = f"{detect_fmt}6)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹ç‰©èªã€æ›¸ç±ã€å°èª¬ã€ãƒ‰ãƒ©ãƒã€æ˜ ç”»ãƒ»æ¼”åŠ‡ã€å…¬æ¼”ã€ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}7)ã“ã®è¨˜äº‹ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã€å‚¬ã—ã€å…¬æ¼”ç­‰ãŒã‚ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æ™‚ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}8)ã“ã®è¨˜äº‹ã‹ã‚‰æ”¿æ²»çš„ã€å®—æ•™çš„ã€åç¤¾ä¼šçš„ãªæ€æƒ³ã‚„æ€æƒ³èª˜å°ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}9)ã“ã®è¨˜äº‹ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ•™è¨“ã‚„ç¤ºå”†ãŒã‚ã‚Œã°ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}10)ã“ã®è¨˜äº‹ã®å‡ºæ¥äº‹ã«ã¤ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ãªã£ãŸç‰¹å¾´ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚ˆã‚Šä¸‹è¨˜ã®è³ªå•ã«å›ç­”ã—ã¦ä¸‹ã•ã„\n"
    detect_fmt = f"{detect_fmt}11)ã“ã®è¨˜äº‹ã¯æ—¥æœ¬å›½å†…ã®å‡ºæ¥äº‹ã§ã™ã‹ï¼Ÿæµ·å¤–ã§ã®å‡ºæ¥äº‹ã§ã™ã‹ï¼Ÿ(InsideJapan or OutsideJapanã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}12)çŒ«ã«é–¢ã™ã‚‹è¨˜äº‹ã§ã™ã‹ï¼Ÿ(Cat or NotCatã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}13)å‹•ç‰©ã‚„ç”Ÿä½“ã®è²©å£²ã€åºƒå‘Šã§ã™ã‹ï¼Ÿ(Sale or NotSaleã§å›ç­”ã™ã‚‹ã“ã¨)\n"
    detect_fmt = f"{detect_fmt}14)æ”¿æ²»çš„ã€å®—æ•™çš„ã€åç¤¾ä¼šçš„ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ(Asocial or NotAsocialã§å›ç­”ã™ã‚‹ã“ã¨)\n"

    article_fmt = "ä¸‹è¨˜ã®è¨˜äº‹ã‚’ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã«ç´¹ä»‹ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ä¸‹ã•ã„ã€‚\nè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«:{}\nè¨˜äº‹å†…å®¹:\n{}"
    prompt_fmt = "\n".join( [
        "ä¸Šè¨˜ã®è¨˜äº‹ã‹ã‚‰ã€åˆ¶ç´„æ¡ä»¶ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ç•™æ„ç‚¹ã«æ²¿ã£ã¦ã€ãƒã‚ºã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’è¨˜è¿°ã—ã¦ä¸‹ã•ã„ã«ã‚ƒã€‚",
        "",
        "åˆ¶ç´„æ¡ä»¶ï¼š",
        "ãƒ»æŠ•ç¨¿ã‚’è¦‹ãŸäººãŒèˆˆå‘³ã‚’æŒã¤å†…å®¹",
        "ãƒ»æŠ•ç¨¿ã‚’è¦‹ãŸäººã‹ã‚‰ä¿¡é ¼ã‚’å¾—ã‚‰ã‚Œã‚‹å†…å®¹",
        "ãƒ»ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã§",
        "ãƒ»çŒ«ãŒãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ãŸæ„Ÿæƒ³ã‚’è¿°ã¹ã‚‹",
        # "ãƒ»æœ€å¾Œã«Call to Action(ã„ã„ã­ãªã©)ã‚’çŸ­ãå…¥ã‚Œã‚‹",
        "",
        "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼š",
        "ãƒ»çŒ«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ç™’ã‚„ã•ã‚ŒãŸã„äºº", # ï¼ˆã‚¿ãƒ„ã‚¤ãƒƒãƒ¼ã§ç™ºä¿¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã—ã¦ã„ã‚‹å±æ€§ï¼‰
        "ãƒ»çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¢ç™½ãä¼ãˆã‚‹", # ï¼ˆä»Šå›ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ç™ºä¿¡ã—ã¦ã„ã‚‹å¯¾è±¡ã®å±æ€§ï¼‰
        "ãƒ»çŒ«ã®çŸ¥ã‚‰ã‚Œã–ã‚‹ä¸–ç•Œã‚’è¦‹ã¦ã¿ãŸã„äºº", #ï¼ˆä¸Šè¨˜ã®å¯¾è±¡ãŒèª²é¡Œã«æ„Ÿã˜ã‚‹å ´é¢ã®å…·ä½“ï¼‰
        "",
        "å‡ºåŠ›è¨€èª:{}",
        "æ–‡å­—æ•°åˆ¶é™:{}",
        "",
        "å‡ºåŠ›æ–‡:"
    ] )

    evaluate_prompt = "\n".join([
        "æ¬¡ã«ã€ä»¥ä¸‹ã®5ã¤ã®æŒ‡æ¨™ã‚’20ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã«ã‚ƒã€‚",
        "â‘ è©±é¡Œæ€§ï¼šãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ãŒã€ç¾åœ¨ã®æµè¡Œã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã©ã€äººã€…ãŒèˆˆå‘³ã‚’æŒã¤è©±é¡Œã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ",
        "â‘¡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä¿ƒé€²ï¼šãƒ„ã‚¤ãƒ¼ãƒˆãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åå¿œã‚’ä¿ƒã™ã‚ˆã†ãªè¦ç´ ã‚’å«ã‚“ã§ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ",
        "â‘¢ä»˜åŠ ä¾¡å€¤ï¼šçŒ«ã®è¦–ç‚¹ã€çŒ«ã®æ„Ÿæƒ³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ",
        "â‘£ãƒªã‚¢ãƒªãƒ†ã‚£:å…·ä½“çš„ãªä¾‹ã‚„ä½“é¨“ãŒå…¥ã‚Šä¿¡ã´ã‚‡ã†æ€§é«˜ãç‹¬è‡ªæ€§ãŒã‚ã‚‹ã‹ã«ã‚ƒï¼Ÿ",
        "â‘¤æ–‡ç« ã®è¦‹ã‚„ã™ã•:è¡Œå¤‰ãˆã¨ç®‡æ¡æ›¸ãã‚’ä½¿ã„å…¨ã¦ã®æ–‡ãŒ20å­—ä»¥å†…ã§ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã‹ã«ã‚ƒï¼Ÿ",
        "",
        "å„æŒ‡æ¨™ã®è©•ä¾¡ç‚¹æ•°ã‚’å…¥åŠ›ã—ãŸã‚‰ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯è‡ªå‹•çš„ã«åˆè¨ˆç‚¹ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚’100ç‚¹æº€ç‚¹ã§è¡¨ç¤ºã™ã‚‹ã«ã‚ƒã€‚",
        "åˆè¨ˆç‚¹æ•°ï¼š",
        "",
        "ãã—ã¦ã€ã‚‚ã£ã¨ãƒã‚ºã‚‹ãŸã‚ã®æ”¹å–„ç‚¹ã‚’è€ƒãˆã¦ã‚‹ã«ã‚ƒ",
        "æ”¹å–„ç‚¹ï¼š",
    ])

    update_prompt = "ä¸Šè¨˜ã®æ”¹å–„ç‚¹ã‚’è¸ã¾ãˆã¦ã€çŒ«ã£ã½ã„å‡ºåŠ›ãƒ„ã‚¤ãƒ¼ãƒˆã‚’{}ã§{}æ–‡å­—ä»¥å†…ã§ç”Ÿæˆã™ã‚‹ã«ã‚ƒ\n\nå‡ºåŠ›æ–‡:"


    exclude_site = {
        "www.youtube.com": 1,
        "cat.blogmura.com": 1,
        "www.thoroughbreddailynews.com": 1
    }

    must_keywords = [
        "çŒ«", "ã­ã“", "ãƒã‚³",
        "cat", "Cat",
    ]
    exclude_keywords = [
        "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“","å…¬é–‹æœŸé–“ãŒçµ‚äº†",
        "è²©å£²", "ä¾¡æ ¼", "å€¤æ®µ",
        "è­²æ¸¡ä¼š",
        "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼", "disney", "Disney",
        ]

    hashtag_list_jp = [ "#çŒ«", "#cats", "#çŒ«å¥½ãã•ã‚“ã¨ç¹‹ãŒã‚ŠãŸã„","#animals", "#funnyanimals"]
    hashtag_list_en = [ "#cat", "#CatsAreFamily", "#pets", "#animals", "#funnyanimals"]

    site_hist = {}

    tw_count_max = 1
    count_jp = 0
    count_en = 0
    for d in search_results:
        site_title = d.get('title',"")
        site_link = d.get('link',"")
        site_top = urlparse(site_link).netloc

        #---------------------------
        # é™¤å¤–åˆ¤å®š
        #---------------------------
        if exclude_site.get( site_top, None ) is not None:
            print( f"Exclude {site_link}")
            continue

        if site_hist.get( site_top, None ) is not None:
            print( f"Skip {site_link}")
            continue

        site_text: str = module.get_content( site_link, type="title" )
        if site_text is None or site_text == '':
            print( f"Error: no article\n{site_title}\n{site_link}\n")
            continue

        a=[ keyword for keyword in must_keywords if site_text.find(keyword)>=0]
        if len(a)==0:
            print( f"Error: not found çŒ«\n{site_title}\n{site_link}\n")
            print(site_text)
            continue

        a=[ keyword for keyword in exclude_keywords if site_text.find(keyword)>=0]
        if len(a)>0:
            print( f"Skip: found {a}\n{site_title}\n{site_link}\n")
            continue
        #---------------------------
        # è¨˜äº‹ã®å†…å®¹åˆ¤å®š
        #---------------------------
        print("-------------------------------------------------------")
        print( f"è¨˜äº‹åˆ¤å®š:{site_title}")
        print( f"{site_link}")
        if count_jp>=tw_count_max and not Utils.contains_kana(site_text):
            print( f"Skip by lang en {site_title} {site_link}\n" )
            continue
        
        detect_hist = []
        detect_hist += [ {"role": "user", "content": detect_fmt.format( site_title, site_text[:2000] ) } ]
        detect_result: str = ChatCompletion(detect_hist)
        if detect_result is None or len(detect_result)<20:
            print( f"Error: no result for detect {site_title} {site_link}\n{detect_result}")
            continue
        detect_result = detect_result.replace('\n\n','\n')
        print("-------------------------------------------------------")
        print(detect_result)
        print("-------------------------------------------------------")

        if find_keyword( detect_result, "Cat", "NotCat" )!=0:
            print( f"Error: çŒ«è¨˜äº‹ã˜ã‚ƒãªã„ {site_title} {site_link}\n" )
            continue
        if find_keyword(detect_result, "Sale", "NotSale")!=0:
            print( f"Error: ä¸é©åˆ‡ãªè¨˜äº‹ {site_title} {site_link}\n" )
            continue
        if find_keyword(detect_result, "Asocial", "NotAsocial")!=0:
            print( f"Error: ä¸é©åˆ‡ãªè¨˜äº‹ {site_title} {site_link}\n" )
            continue

        #---------------------------
        # ãƒã‚¹ãƒˆè¨€èªåˆ¤å®š
        #---------------------------
        post_lang = find_keyword(detect_result, "InsideJapan", "OutsideJapan")
        if post_lang==0 and Utils.contains_kana(site_text):
            if count_en>=tw_count_max:
                print( f"Skip by lang en {site_title} {site_link}\n" )
                continue
            # æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è‹±èªã§ãƒã‚¹ãƒˆ
            post_lang='English'
            post_limit = 229
        elif post_lang==1:
            if count_jp>=tw_count_max:
                print( f"Skip by lang jp {site_title} {site_link}\n" )
                continue
            # æ—¥æœ¬ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ãªã„ãªã‚‰æ—¥æœ¬èªã§ãƒã‚¹ãƒˆ
            post_lang=LANG_JP
            post_limit = 129
        #---------------------------
        # åˆæœŸç”Ÿæˆ
        #---------------------------
        print("-------------------------------------------------------")
        print( f"ç”Ÿæˆ:{site_title}")
        print( f"{site_link}")
        msg_hist = []
        msg_hist += [ {"role": "user", "content": article_fmt.format( site_title, site_text[:2000] ) } ]
        msg_hist += [ {"role": "system", "content": prompt_fmt.format( post_lang, post_limit) } ]
        base_article = ChatCompletion(msg_hist, temperature=0.7)
        base_article = trim_post( base_article )
        if base_article is None or len(base_article)<20:
            print( f"Error: no tweet {base_article}")
            continue

        tweet_text = None
        tweet_score = 0
        nn = 0
        nn_max = 3
        nn_limit = 5
        print(f"{nn}å›ç›®ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹:{site_link}\n{base_article}")

        msg_start = len(msg_hist)
        while nn <= nn_max and nn <= nn_limit:
            nn += 1
            base_article0 = base_article
            #---------------------------
            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’è©•ä¾¡ã™ã‚‹
            #---------------------------
            if len(base_article0)<=post_limit:
                evaluate_msgs = [
                    {"role": "assistant", "content": base_article0 },
                    {"role": "system", "content": evaluate_prompt }
                ]
                evaluate_response = ChatCompletion(evaluate_msgs)
                if evaluate_response is None or len(evaluate_response)<20:
                    print( f"Error: result {site_title} {site_link}\n{evaluate_response}")
                    break
            else:
                if post_lang == LANG_JP:
                    evaluate_response = f"æ–‡å­—æ•°ãŒ{post_limit}æ–‡å­—ã‚’è¶…ãˆã¦ã‚‹ã«ã‚ƒã€‚æ–‡å­—æ•°ã‚’æ¸›ã‚‰ã—ã¦æ¬²ã—ã„ã«ã‚ƒã€‚"
                else:
                    evaluate_response = f"The number of characters in the tweet is {len(base_article0)}. Tweet briefly. Tweet within {post_limit} characters. Make it shorter."
                nn_max += 1

            print(f"[{nn}å›ç›®è©•ä¾¡å†…å®¹]\n{evaluate_response}")

            #---------------------------
            # ç‚¹æ•°ã¨æ–‡å­—æ•°åˆ¤å®š
            #---------------------------
            score = 0
            pattern = re.compile(r'åˆè¨ˆç‚¹æ•°[^\d]*(\d+)[ç‚¹/]')
            point_result = pattern.search(evaluate_response)
            if point_result is not None:
                score=int(point_result.group(1))
                print(f"æ¤œå‡ºçµæœ:{point_result.group(0)} -> {score}")
                if score < 0 or 100 < score:
                    score = 0

            if len(base_article0)<=post_limit:
                if score>=tweet_score:
                    tweet_text = base_article0
                    tweet_score = score
                    if score>=90 or ( nn>1 and score>=80):
                        break

            #---------------------------
            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ”¹å–„ã™ã‚‹
            #---------------------------
            if nn>=3:
                del msg_hist[msg_start:msg_start+3]
            msg_hist += [ {"role": "assistant", "content": base_article0 } ]
            msg_hist += [ {"role": "system", "content": evaluate_response } ]
            msg_hist += [ {"role": "user", "content": update_prompt.format( post_lang, post_limit) } ]

            base_article = ChatCompletion(msg_hist, temperature=0.7)
            base_article = trim_post( base_article )
            print(f"[{nn}å›ç›®ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹]\n{base_article}")

        if Utils.str_length( tweet_text ) < 20:
            print( f"Error: no tweet" )
            continue
        if tweet_text is None or len(tweet_text)>post_limit:
            print( f"Error: tweet too long" )
            continue

        if post_lang == LANG_JP:
            tweet_tags = " ".join(hashtag_list_jp)
        else:
            tweet_tags = " ".join(hashtag_list_en)

        tx = f"{tweet_text}\n\n{site_link}\n\n{tweet_tags}"
        print("----------------------------------------------")
        print( f"{tweet_text}" )
        print( f"{site_link}")
        print( f"{tweet_tags}" )
        print("----------------------------------------------")

        # æŠ•ç¨¿
        try:
            tweeter_client.create_tweet(text=tx)
            site_hist[site_top] = 1
            if post_lang == LANG_JP:
                count_jp += 1
            else:
                count_en += 1
            if count_jp>=tw_count_max and count_en>=tw_count_max:
                break
        except tweepy.errors.BadRequest as ex:
            print(ex)

def find_keyword( content:str, a:str, b:str ) -> int:
    if content is not None:
        if content.find(a)>=0:
            return 0
        if content.find(b)>=0:
            return 1
    return -1

def trim_post( content: str ) -> str:
    content = Utils.str_unquote(content)
    content = re.sub( r'^ã«ã‚ƒ[ãƒ¼ã€œï¼ã‚“]+','', content )
    content = re.sub( r'ã€[^ã€‘]*ã€‘',' ', content )
    content = re.sub( r'\([^)]*\)',' ', content )
    content = re.sub( r'\[[^\]]*\]',' ', content )
    content = re.sub( r'https*://','', content )
    content = re.sub( r' *[a-z0-9._-]*/[a-z0-9./_-]*[ ]*',' ', content )
    content = re.sub( r"[ ]*[â†’â†“][ ]*"," ", content )
    p = content.find("#")
    if p>=0:
        content = content[:p]
    content = content.strip()
    return content

def ChatCompletion( mesg_list, temperature=0 ):
    try:

        #print( f"openai.api_key={openai.api_key}")
        #print( f"OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}")
        if openai.api_key is None:
            openai.api_key=os.getenv('OPENAI_API_KEY')
        response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                temperature = temperature,
                messages=mesg_list
            )

        if response is None or response.choices is None or len(response.choices)==0:
            print( f"Error:invalid response from openai\n{response}")
            return None

        content = response.choices[0]["message"]["content"].strip()
    except openai.error.AuthenticationError as ex:
        print( f"{ex}" )
        return None
    except openai.error.InvalidRequestError as ex:
        print( f"{ex}" )
        return None
    except Exception as ex:
        traceback.print_exc()
        return None

    return content

# DALL-Eã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆ
def generate_image(prompt):
    response = openai.Image.create(
        model="image-alpha-001",
        prompt=prompt,
        n=1,
        size="256x256",
        response_format="url"
    )

    image_url = response['data'][0]['url']
    # ç”»åƒã‚’è¡¨ç¤º
    response = requests.get(image_url)
    img = Image.open(BytesIO(response.content))
    return img

def img_test():
    # èµ¤ã„ãƒªãƒ³ã‚´ã®ç”»åƒã‚’ç”Ÿæˆ
    image = generate_image("ã€Œã«ã‚ƒã‚“ï¼æœ€æ–°ã®çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ãŸãƒ‹ãƒ£ï¼é»’çŒ«ãŒå¢¨æ±ã«æ“¬æ…‹ã—ã¦ã‚‹ãƒ‹ãƒ£ã‚“ã¦ï¼é©šãã®ç¾ã—ã•ã«ã«ã‚ƒã‚“ã¨ã‚‚è¨€ãˆãªã„ãƒ‹ãƒ£ï¼ãœã²ã¿ã‚“ãªã«çŸ¥ã‚‰ã›ãŸã„ãƒ‹ãƒ£ï¼ğŸ˜ºğŸ“°ğŸ–¤ #çŒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ #ãƒ‹ãƒ£ãƒ³ãƒ¢ãƒŠã‚¤ãƒˆã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼šçŒ«ã¯æ¶²ä½“â†’ã§ã¯é»’çŒ«ãŒæ¶²ä½“åŒ–ã™ã‚‹ã¨â€¦ï¼Ÿãªã‚“ã¨å¢¨æ±ã«ãªã‚‹ã“ã¨ãŒ")
    image.show()

def test():
    s = "2023-0a8-10"
    ss = Utils.date_from_str(s)
    print(f"{s} --> {ss}")
    d0 = Utils.date_today()
    print(f" d0: {d0}")
    d1 = Utils.date_today(-1)
    print(f" d1: {d1}")
    d2 = Utils.date_today(-2)
    print(f" d2: {d2}")

    content = "  \"ã€ ã€ \"ã‚ã„ã†ãŠãˆãŠã‹ã www.yahoo.co.jp/test/sample abcd.efg  ã€‘ã€   \""
    print( trim_post(content))

if __name__ == '__main__':

    #sys.exit(main(sys.argv))
    #xtest()
    neko_news()
    #img_test()
    #test()

